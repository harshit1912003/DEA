{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import eat # Assuming 'eat.py' contains the EAT class implementation\n",
    "import pandas as pd\n",
    "from models.modelsFDH import FDH # Assuming 'models/modelsFDH.py' contains the FDH class implementation\n",
    "\n",
    "# Set random seed for reproducibility if desired, otherwise remove or comment out\n",
    "# np.random.seed(42)\n",
    "\n",
    "# --- Constants based on the paper ---\n",
    "# Changed TRIALS from 10 to 100 as per paper text (page 13)\n",
    "TRIALS = 100\n",
    "# Changed SAMPLE_SIZES to match paper tables (page 13, 15)\n",
    "SAMPLE_SIZES = [50, 100, 150]\n",
    "\n",
    "# --- Scenario Definitions based on Table 1 (page 13) ---\n",
    "SCENARIOS = {\n",
    "    1: {\"inputs\": 1,\n",
    "        \"outputs\": 1,\n",
    "        \"func\": lambda x: np.log(x[:,0]) + 3, # Use vectorized operations\n",
    "        \"inefficiency\": lambda n: np.abs(np.random.normal(0, 0.4, size=n))},\n",
    "    2: {\"inputs\": 2,\n",
    "        \"outputs\": 1,\n",
    "        # Use vectorized operations\n",
    "        \"func\": lambda x: 0.1*x[:,0] + 0.1*x[:,1] + 0.3*((x[:,0] * x[:,1])**(1/2)),\n",
    "        \"inefficiency\": lambda n: np.abs(np.random.normal(0, 0.4, size=n))},\n",
    "    3: {\"inputs\": 3,\n",
    "        \"outputs\": 1,\n",
    "        # Use vectorized operations\n",
    "        \"func\": lambda x: 0.1*x[:,0] + 0.1*x[:,1] + 0.1*x[:,2] + 0.3*(x[:,0]*x[:,1]*x[:,2])**(1/3),\n",
    "        \"inefficiency\": lambda n: np.abs(np.random.normal(0, 0.4, size=n))},\n",
    "    4: {\"inputs\": 9,\n",
    "        \"outputs\": 1,\n",
    "        # Use vectorized operations - Paper function f(x)=Π(xi^0.1) is ambiguous, using Lee and Cai (2020) interpretation if available or assuming geometric mean variant.\n",
    "        # This assumes f(x) = (Πxi)^(1/9) -> exp( (1/9) * sum(log(xi)) ). Let's use the Cobb-Douglas form Π(x_i ^ (1/9))\n",
    "        \"func\": lambda x: np.prod(x**(1/9), axis=1), # Use x directly as it's (n, 9)\n",
    "        \"inefficiency\": lambda n: np.abs(np.random.normal(0, 0.4, size=n))},\n",
    "    5: { # Multi-output scenario based on Perelman and Santín (2009) referenced in paper\n",
    "        \"inputs\": 2,\n",
    "        \"outputs\": 2,\n",
    "        # The function definition here describes the *relationship*, not a direct generation formula for y.\n",
    "        # Generation is handled separately in generate_data based on the paper's description/likely source method.\n",
    "        \"func\": None, # Mark as None, generation logic is specific\n",
    "        # Inefficiency applied component-wise for multi-output\n",
    "        \"inefficiency\": lambda n: np.abs(np.random.normal(0, 0.4, size=(n, 2))),\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Data Generation Function ---\n",
    "def generate_data(scenario_id, scenario, n):\n",
    "    \"\"\"Generates data for a given scenario and sample size.\"\"\"\n",
    "    x = np.random.uniform(1, 10, size=(n, scenario[\"inputs\"]))\n",
    "    true_frontier_y = np.zeros((n, scenario[\"outputs\"])) # Initialize true frontier output array\n",
    "\n",
    "    # --- Calculate True Frontier Output (f(x)) ---\n",
    "    if scenario_id == 5:\n",
    "        # Multi-output generation needs specific logic (e.g., solving the implicit function)\n",
    "        # Replicating Perelman and Santín (2009) method precisely might be complex.\n",
    "        # Using a simplified placeholder or assumed generation logic if the exact method isn't implemented.\n",
    "        # Placeholder: Generate y1, y2 based on some function of x, ensuring positivity and reasonable scale.\n",
    "        # This part *must* be correctly implemented based on the paper's source method for Scenario 5 results to be valid.\n",
    "        # Example placeholder (likely incorrect w.r.t paper's source):\n",
    "        true_frontier_y[:, 0] = 0.5 * x[:, 0] + 0.3 * x[:, 1] + 2\n",
    "        true_frontier_y[:, 1] = 0.2 * x[:, 0] + 0.6 * x[:, 1] + 1\n",
    "        # --- !!! Crucial: Replace placeholder with correct Scenario 5 generation logic !!! ---\n",
    "\n",
    "    else:\n",
    "        # Single output scenarios (1-4)\n",
    "        true_frontier_y = scenario[\"func\"](x)\n",
    "        # Reshape to (n, 1) if it's not already\n",
    "        if true_frontier_y.ndim == 1:\n",
    "             true_frontier_y = true_frontier_y.reshape(-1, 1)\n",
    "\n",
    "    # --- Generate Inefficiency Term (u) ---\n",
    "    inefficiency = scenario[\"inefficiency\"](n)\n",
    "    # Ensure inefficiency term has the same shape as output for subtraction\n",
    "    if inefficiency.ndim == 1 and scenario[\"outputs\"] > 1:\n",
    "         # This case shouldn't happen with the current SCENARIOS definition but added for robustness\n",
    "         inefficiency = np.tile(inefficiency.reshape(-1, 1), (1, scenario[\"outputs\"]))\n",
    "    elif inefficiency.ndim == 1 and scenario[\"outputs\"] == 1:\n",
    "         inefficiency = inefficiency.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # --- Calculate Observed Output (y = f(x) - u) ---\n",
    "    observed_y = true_frontier_y - inefficiency\n",
    "\n",
    "    # Ensure observed outputs are positive (common practice in production frontiers)\n",
    "    # This step wasn't explicitly mentioned for scenarios 1-4 but often necessary.\n",
    "    # Adjust if the paper implies negative outputs are possible.\n",
    "    observed_y[observed_y < 0] = 1e-6 # Replace negative values with a small positive number\n",
    "\n",
    "    # --- Return true frontier values for metric calculation ---\n",
    "    # Changed return value to include true_frontier_y\n",
    "    return x, observed_y, true_frontier_y\n",
    "\n",
    "# --- Monte Carlo Simulation Function ---\n",
    "def monte_carlo_testing():\n",
    "    \"\"\"Runs the Monte Carlo simulation.\"\"\"\n",
    "    results = {}\n",
    "    discrepancy_counts = {} # To store counts for FDH vs Deep EAT discrepancy (optional, like in paper)\n",
    "\n",
    "    for scenario_id, scenario in SCENARIOS.items():\n",
    "        print(f\"--- Running Scenario {scenario_id} ---\")\n",
    "        discrepancy_counts[scenario_id] = {}\n",
    "        for n in SAMPLE_SIZES:\n",
    "            print(f\"  Sample Size: {n}\")\n",
    "            mse_fdh_list, mse_eat_list = [], []\n",
    "            bias_fdh_list, bias_eat_list = [], []\n",
    "            abs_bias_fdh_list, abs_bias_eat_list = [], []\n",
    "            discrepancies = 0 # Counter for FDH != Deep EAT\n",
    "\n",
    "            for trial in range(TRIALS):\n",
    "                if (trial + 1) % 20 == 0: # Print progress\n",
    "                   print(f\"    Trial {trial + 1}/{TRIALS}\")\n",
    "\n",
    "                # Generate data, now including true_frontier_y\n",
    "                x, y_observed, y_true_frontier = generate_data(scenario_id, scenario, n)\n",
    "\n",
    "                # --- FDH Estimation ---\n",
    "                # Ensure y_observed is 2D\n",
    "                if y_observed.ndim == 1:\n",
    "                    y_observed_fdh = y_observed.reshape(-1, 1)\n",
    "                else:\n",
    "                    y_observed_fdh = y_observed\n",
    "\n",
    "                # Assuming FDH class takes x, y and calculates projections\n",
    "                # Note: FDH itself doesn't estimate f(x) directly, it finds efficient peers.\n",
    "                # The 'estimated frontier' for FDH at point xi is the maximum output achievable\n",
    "                # by units dominating xi. For comparison, we often use the radial projection.\n",
    "                try:\n",
    "                    fdh_model = FDH(x, y_observed_fdh)\n",
    "                    # Assuming fdh_output_vrs returns efficiency scores or projections\n",
    "                    # If it returns efficiency scores 'eff': y_fdh_est = y_observed / eff (output oriented)\n",
    "                    # If it returns projected points: y_fdh_est = projected_y\n",
    "                    # Let's assume it gives projected points 'y_proj' as a common output format\n",
    "                    # Need to confirm what `fdh_output_vrs` actually returns\n",
    "                    # Placeholder: Assuming it returns a dataframe with projections or scores\n",
    "                    # Using radial projection calculation here as it's common\n",
    "                    # This part *requires* knowing the exact output of your FDH implementation\n",
    "                    # Example: If it returns efficiency scores:\n",
    "                    # fdh_results_df = fdh_model.fdh_output_vrs() # Assuming returns DataFrame\n",
    "                    # efficiency_scores = fdh_results_df[\"efficiency\"].values\n",
    "                    # # Handle potential division by zero or very small efficiency scores if any\n",
    "                    # efficiency_scores[efficiency_scores < 1e-9] = 1e-9\n",
    "                    # # Calculate output-oriented projection\n",
    "                    # y_fdh_est = y_observed_fdh * efficiency_scores.reshape(-1, 1)\n",
    "                    \n",
    "                    # Using original code's structure assuming it calculates projection correctly\n",
    "                    fdh_results_df = fdh_model.fdh_output_vrs()\n",
    "                    y_fdh_est = y_observed_fdh * fdh_results_df[\"efficiency\"].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in FDH Trial {trial+1} (Scenario {scenario_id}, n={n}): {e}\")\n",
    "                    # Skip trial or handle error appropriately\n",
    "                    y_fdh_est = np.full_like(y_true_frontier, np.nan) # Fill with NaN on error\n",
    "\n",
    "\n",
    "                # --- EAT Estimation ---\n",
    "                # Prepare DataFrame for EAT class\n",
    "                x_cols = [f\"x[{i}]\" for i in range(scenario[\"inputs\"])]\n",
    "                y_cols = [f\"y[{i}]\" for i in range(scenario[\"outputs\"])]\n",
    "                df = pd.DataFrame(x, columns=x_cols)\n",
    "                df[y_cols] = y_observed # Use observed y for training EAT\n",
    "\n",
    "                try:\n",
    "                    # Using numStop=5 as suggested by Breiman et al. (1984) and implicit in paper (page 6)\n",
    "                    # Using fold=10 (common default for cross-validation) if 5 isn't standard in your lib\n",
    "                    eat_model = eat.EAT(\n",
    "                        df, x_cols, y_cols,\n",
    "                        numStop=5, # Changed numStop based on paper discussion\n",
    "                        fold=10) # Standard CV folds, paper doesn't specify\n",
    "                    eat_model.fit() # Fit includes pruning\n",
    "\n",
    "                    # Predict using the same x values\n",
    "                    data_pred = df.loc[:, x_cols]\n",
    "                    y_eat_pred_df = eat_model.predict(data_pred) # Assuming predict takes df and returns df\n",
    "\n",
    "                    # Extract predicted columns (assuming names like 'p_y[0]', 'p_y[1]', ...)\n",
    "                    pred_y_cols = [f\"p_y[{i}]\" for i in range(scenario[\"outputs\"])]\n",
    "                    y_eat_est = y_eat_pred_df[pred_y_cols].values\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in EAT Trial {trial+1} (Scenario {scenario_id}, n={n}): {e}\")\n",
    "                    y_eat_est = np.full_like(y_true_frontier, np.nan) # Fill with NaN on error\n",
    "\n",
    "                # --- Calculate Metrics ---\n",
    "                # Pass the TRUE frontier values for comparison\n",
    "                mse_fdh, bias_fdh, abs_bias_fdh = calculate_metrics(y_true_frontier, y_fdh_est)\n",
    "                mse_fdh_list.append(mse_fdh)\n",
    "                bias_fdh_list.append(bias_fdh)\n",
    "                abs_bias_fdh_list.append(abs_bias_fdh)\n",
    "\n",
    "                mse_eat, bias_eat, abs_bias_eat = calculate_metrics(y_true_frontier, y_eat_est)\n",
    "                mse_eat_list.append(mse_eat)\n",
    "                bias_eat_list.append(bias_eat)\n",
    "                abs_bias_eat_list.append(abs_bias_eat)\n",
    "\n",
    "\n",
    "            # --- Store Average Results for (Scenario, n) ---\n",
    "            # Added checks for NaN in case of errors during trials\n",
    "            results[(scenario_id, n)] = {\n",
    "                \"FDH_MSE\": np.nanmean(mse_fdh_list),\n",
    "                \"EAT_MSE\": np.nanmean(mse_eat_list),\n",
    "                \"FDH_Bias\": np.nanmean(bias_fdh_list),\n",
    "                \"EAT_Bias\": np.nanmean(bias_eat_list),\n",
    "                \"FDH_AbsBias\": np.nanmean(abs_bias_fdh_list),\n",
    "                \"EAT_AbsBias\": np.nanmean(abs_bias_eat_list),\n",
    "            }\n",
    "            # Optional: Calculate discrepancy percentage like in paper's Table 2\n",
    "            # discrepancy_percentage = (discrepancies / (n * TRIALS)) * 100\n",
    "            # results[(scenario_id, n)][\"Discrepancy_Percent\"] = discrepancy_percentage\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Metrics Calculation Function ---\n",
    "def calculate_metrics(true_values, estimates):\n",
    "    \"\"\"Calculates MSE, Bias, and Absolute Bias.\"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    true_values = np.asarray(true_values)\n",
    "    estimates = np.asarray(estimates)\n",
    "\n",
    "    # Handle potential NaNs from failed trials\n",
    "    mask = ~np.isnan(estimates) & ~np.isnan(true_values)\n",
    "    if np.sum(mask) == 0: # All estimates failed\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    valid_true = true_values[mask]\n",
    "    valid_est = estimates[mask]\n",
    "\n",
    "    # Calculate metrics only on valid pairs\n",
    "    mse = np.mean((valid_est - valid_true) ** 2)\n",
    "    bias = np.mean(valid_est - valid_true)\n",
    "    abs_bias = np.mean(np.abs(valid_est - valid_true))\n",
    "\n",
    "    return mse, bias, abs_bias\n",
    "\n",
    "# --- Run Simulation and Print Results ---\n",
    "results = monte_carlo_testing()\n",
    "\n",
    "print(\"\\n--- Monte Carlo Simulation Results ---\")\n",
    "# Prepare data for table display\n",
    "table_data = []\n",
    "for (scenario_id, n), metrics in results.items():\n",
    "    # Calculate percentage improvement for EAT over FDH (Reduction)\n",
    "    mse_reduction = ((metrics['FDH_MSE'] - metrics['EAT_MSE']) / metrics['FDH_MSE']) * 100 if metrics['FDH_MSE'] else 0\n",
    "    # Bias reduction - tricky sign convention, use absolute bias reduction\n",
    "    abs_bias_reduction = ((metrics['FDH_AbsBias'] - metrics['EAT_AbsBias']) / metrics['FDH_AbsBias']) * 100 if metrics['FDH_AbsBias'] else 0\n",
    "\n",
    "    table_data.append({\n",
    "        \"Scenario\": scenario_id,\n",
    "        \"N\": n,\n",
    "        \"FDH MSE\": metrics['FDH_MSE'],\n",
    "        \"EAT MSE\": metrics['EAT_MSE'],\n",
    "        \"MSE Reduction (%)\": mse_reduction,\n",
    "        \"FDH Bias\": metrics['FDH_Bias'],\n",
    "        \"EAT Bias\": metrics['EAT_Bias'],\n",
    "        \"FDH AbsBias\": metrics['FDH_AbsBias'],\n",
    "        \"EAT AbsBias\": metrics['EAT_AbsBias'],\n",
    "        \"AbsBias Reduction (%)\": abs_bias_reduction\n",
    "        # Add \"Discrepancy (%)\": metrics.get(\"Discrepancy_Percent\", np.nan) if calculated\n",
    "    })\n",
    "\n",
    "# Display results in a pandas DataFrame\n",
    "results_df = pd.DataFrame(table_data)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x) # Format floats\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "# results_df.to_csv(\"monte_carlo_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
